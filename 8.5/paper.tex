\documentclass{ansarticle-preprint}
%\usepackage{ucs}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%\usepackage{cite}
\usepackage{anslistings}
\usepackage{multicol}

\usepackage{pgfplots}
\usepackage{pgfplotstable}

\usepackage{fontenc}
\usepackage{graphicx}

\newcommand{\specialword}[1]{\texttt{#1}}
\newcommand{\dealii}{{\specialword{deal.II}}}
\newcommand{\pfrst}{{\specialword{p4est}}}
\newcommand{\trilinos}{{\specialword{Trilinos}}}
\newcommand{\aspect}{\specialword{Aspect}}
\newcommand{\petsc}{\specialword{PETSc}}
\newcommand{\cmake}{{\specialword{CMake}}}
\newcommand{\autoconf}{{\specialword{autoconf}}}

%
% Author list -- please add yourself in both places below (in
%                alphabetical order) if you think that your
%                contributions to the last release warrant this
%

\hypersetup{
  pdfauthor={
    Wolfgang Bangerth,
    Denis Davydov,
    Timo Heister,
    Luca Heltai,
    Martin Kronbichler,
    Matthias Maier,
    Jean-Paul Pelteret
  },
  pdftitle={The deal.II Library, Version 8.5, 2017},
}

\title{The \dealii{} Library, Version 8.5}

\author[1]{Wolfgang Bangerth}
\affil[1]{Department of Mathematics, Colorado State University, Fort
  Collins, CO 80523-1874, USA.
    {\texttt{bangerth@colostate.edu}}}

\author[2]{Denis Davydov}
\affil[2]{Chair of Applied Mechanics, University of
     Erlangen-Nuremberg, Egerlandstr.\ 5, 91058 Erlangen, Germany.
    {\texttt{denis.davydov@fau.de}}}

\author[3]{Timo Heister}
\affil[3]{Mathematical Sciences,
  O-110 Martin Hall,
  Clemson University,
  Clemson, SC 29634, USA.
  {\texttt{heister@clemson.edu}}}

\author[4]{Luca Heltai}
\affil[4]{SISSA,
  International School for Advanced Studies,
  Via Bonomea 265,
  34136, Trieste, Italy
{\texttt{luca.heltai@sissa.it}}}

\author[5]{Martin Kronbichler}
\affil[5]{Institute for Computational Mechanics,
  Technical University of Munich,
  Boltzmannstr.~15, 85748 Garching, Germany.
  {\texttt{kronbichler@lnm.mw.tum.de}}}

\author[6]{Matthias Maier}
\affil[6]{School of Mathematics,
  University of Minnesota,
  127 Vincent Hall, 206 Church Street SE,
  Minneapolis, MN 55455, USA,
  {\texttt{msmaier@umn.edu}}}

\author[7]{Jean-Paul Pelteret}
\affil[7]{Chair of Applied Mechanics,
  University of Erlangen-Nuremberg,
  Egerlandstr.\ 5,
  91058 Erlangen,
  Germany.
  {\texttt{jean-paul.pelteret@fau.de}}}

\renewcommand{\labelitemi}{--}




\begin{document}
\maketitle

\begin{abstract}
  This paper provides an overview of the new features of the finite element
  library \dealii{} version 8.5.
\end{abstract}


\section{Overview}

\marginpar{Update date}
\dealii{} version 8.5.0 was released March 11, 2017. This paper provides an
overview of the new features of this release and serves as a citable
reference for the \dealii{} software library version 8.5. \dealii{} is an
object-oriented finite element library used around the world in the
development of finite element solvers. It is available for free under the
GNU Lesser General Public License (LGPL) from the \dealii{} homepage at
\url{http://www.dealii.org/}.

The major changes of this release are:
\marginpar{Add one sentence to each of the following.}
\begin{itemize}
\item The \texttt{CellDataStorage} class provides a mechanism to store user-defined data on each cell.
This data is treated as a first-class citizen to \dealii{} and, when used in conjunction with other classes, can be shipped to other MPI cores during mesh refinement and repartitioning. 

\item The \texttt{MappingManifold} class provides mappings between the
  reference cell and a mesh cell that is ``exact'' in the sense that
  the edges, faces, and the interior of the mesh cell matches an
  underlying manifold. This is opposed to using the usual polynomial
  approximations of the manifold.
  TODO: Luca

\item Trilinos linear operators?
  TODO: Jean-Paul and Matthias

\item A dedicated physics module has been created to provide some standard definitions and operations used in continuum physics.

\item FE\_Enriched

\item FESeries namespace, use in step-27

\item New tutorial programs step-55, step-56, and step-57,
  demonstrating parallel solvers for the Stokes equations, a multigrid
  method for the Stokes equation, and methods for the nonlinear
  Navier-Stokes equations, respectively. In addition, the step-37
  tutorial program has been rewritten to support parallel
  computations, and the step-44 tutorial now uses the facilities of
  the physics module mentioned above.

\item New code gallery programs demonstrating (i) quasi-static
  visco-elastic material behavior, (ii) multiphase Navier-Stokes flow,
  (iii) the evolution of global-scale topography on planetary bodies,
  (iv) goal-oriented elastoplasticity.


\item More than 230 other features and bugfixes.
\end{itemize}
The more important ones of these will be detailed in the following section.
Information on how to cite \dealii{} is provided in Section \ref{sec:cite}.


\section{Significant changes to the library}

This release of \dealii{} contains a number of large and significant changes
that will be discussed in the following sections. It of course also contains a
vast number of smaller changes and added functionality; the details of these
can be found
\href{https://www.dealii.org/8.5.0/doxygen/deal.II/changes_between_8_4_and_8_5.html}{in the file that lists all changes for this release} (see \cite{changes84})
and that is linked to from the web site of each release as well as the
release announcement.


\subsection{The \texttt{CellDataStorage} class and friends}

The \texttt{CellDataStorage} class is an integrated mechanism to safely store user-defined data, such as that required at each computation point, within each cell.
Although the same was previously achieved through the use of a cell \texttt{user\_pointer}, it required users to manage this data themselves.
As this data is now treated as a first-class citizen to \dealii{}, through the abstract \texttt{TransferableQuadraturePointData} we have provided a generic interface to facilitate several low-level operations that were previously tedious or technically challenging to implement.
For instance, the \texttt{parallel::distributed::ContinuousQuadratureDataTransfer} class assists in the transfer of arbitrary data (that is continuous within a cell) stored at quadrature points when performing h-adaptive refinement of \texttt{parallel::distributed::Triangulation}.
Not only does it perform an $\mathcal{L}^2$ projection of the specified user data between quadrature points, but it also ships the data automatically between MPI processes.

\subsection{The \texttt{MappingManifold} class}

TODO: Luca

\subsection{TODO: Linear operators with Payload and Trilinos}

TODO: Jean-Paul and Matthias

\subsection{The physics module}

A dedicated physics module has been created to facilitate the implementation of functions and classes that relate to continuum physics, physical fields and material constitutive laws.
To date, it includes transformations of scalar or tensorial quantities between any two configurations, and some definitions typically utilized in both linear and finite-strain nonlinear elasticity.

The \verb!Physics::Transformations! namespace offers push-forward and pull-back operations in the context of contravariant, covariant and Piola transformations, as well as rotation operations for Euclidean space.
In the \verb!Physics::Elasticity::Kinematics! namespace, a selection of deformation, strain tensors and strain rate tensors are defined.
The \verb!Physics::Elasticity::StandardTensors! class provides some frequently used second and fourth order metric tensors, and defines some referential and spatial projection operators and tensor derivatives as commonly required in the definition of material laws.

\subsection{Scalability of geometric multigrid framework}

For the new release, the geometric multigrid facilities in \dealii{} have been
thoroughly overhauled regarding their scalability on large-scale parallel
computers. During this process, a geometric multigrid implementation based on
the fast matrix-free kernels from \cite{KronbichlerKormann2012} has been
benchmarked up to 147,456 cores. The fast matrix-vector product revealed
several scalability bottlenecks, including unnecessary inner products inside
the Chebyshev smoother and $\mathcal O(n_\text{levels})$ global communication
steps during the restriction process rather than only the single global
communication step that is necessary when going to the coarest grid. New
matrix-free transfer implementations called \texttt{MGTransferMatrixFree} were
devised that can replace the matrix-based \texttt{MGTransferPrebuilt} class
for tensor product elements. Besides better scalability than the Trilinos
Epetra matrices underlying the latter, the matrix-free transfer is also a much
better for high-order elements with complexity per degree of freedom of
$\mathcal O(d p)$ in the polynomial degree $p$ in $d$ dimensions rather than
$\mathcal O(p^d)$ for the matrix-based approach.

\begin{figure}
\pgfplotstableread{
nprocs     fem256k    fem2m     fem16m    fem128m  fem1g    fem8g
16         0.640934   4.97684   nan       nan      nan      nan
32         0.325741   2.51771   nan       nan      nan      nan
64         0.1645     1.2823    nan       nan      nan      nan
128        0.090898   0.658832  5.00366   nan      nan      nan
256        0.059922   0.339999  2.56216   nan      nan      nan
512        0.0455449  0.176482  1.29986   nan      nan      nan
1024       0.0368049  0.099691  0.67364   6.48155  nan      nan
2048       0.0348921  0.069573  0.356066  2.59601  nan      nan
4096       0.0367949  0.056833  0.19293   1.3251   nan      nan
8192       0.033958   0.045350  0.110485  0.790214 5.50143  nan
16384      0.0379629  0.049351  0.099904  0.424692 2.81479  nan
32768      0.0461671  0.051276  0.077546  0.229114 1.53091  nan
65536      0.0466189  0.058941  0.075194  0.127353 0.819    6.2909
147456     nan        nan       nan       0.087949 0.43213  2.90856
}\scalinglarge
\pgfplotstableread{
nprocs    newdg256k    newdg2m   newdg16m olddg256k  olddg2m   olddg16m
28        1.4398       12.2014   nan      1.458725   12.25674  nan
56        0.6987        6.2015   nan      0.721993   6.266567  nan
112       0.3352        3.1782   nan      0.353412   3.263585  nan
224       0.1888        1.5687   12.8235  0.183643   1.611967  13.03189
448       0.0853        0.7686   6.5649   0.105388   0.805367  6.700762
896       0.0478        0.3537   3.3685   0.063270   0.383875  3.446427
1792      0.0317        0.1717   1.7061   0.046065   0.199738  1.762310
3584      0.0235        0.1079   0.8580   0.042032   0.123113  0.902782
7168      0.0207        0.0668   0.4328   0.037678   0.091822  0.462007
14336     0.0183699     0.045095 0.23276  0.038804   0.071644  0.288911
}\scalingHSW
\centering
\definecolor{gnuplot@green}{RGB}{0,158,115}
\begin{tikzpicture}
    \begin{loglogaxis}[
      title style={at={(0.5,0.965)},anchor=north,draw=black,fill=white,font=\scriptsize\bf},
      title={strong and weak scaling, continuous $\mathcal Q_3$ elements},
      width=0.53\textwidth,
      height=0.5\textwidth,
      xlabel={Number of cores},
      ylabel={Solver time [s]},
      x label style={at={(0.5,0.02)}},
      y label style={at={(0.05,0.5)}},
      xtick={32,128,512,2048,8192,32768,147456},
      xticklabels={32,128,512,2048,8192,32k,147k},
      tick label style={font=\scriptsize},
      label style={font=\scriptsize},
      legend style={font=\scriptsize},
      legend pos=south west,
      ymin=5e-3, ymax=15,
      xmin=8, xmax=147456,
      grid
      ]
      \addplot table[x={nprocs}, y={fem8g}] {\scalinglarge};
      \addlegendentry{8B cells};
      \addplot table[x={nprocs}, y={fem1g}] {\scalinglarge};
      \addlegendentry{1B cells};
      \addplot table[x={nprocs}, y={fem128m}] {\scalinglarge};
      \addlegendentry{128M cells};
      \addplot table[x={nprocs}, y={fem16m}] {\scalinglarge};
      \addlegendentry{16M cells};
      \addplot[gnuplot@green,mark=diamond*,mark options={fill=gnuplot@green!40}] table[x={nprocs}, y={fem2m}] {\scalinglarge};
      \addlegendentry{2M cells};
      \addplot[dashed,black] coordinates {
        (8,10)
        (147456,5/9168)
      };
      \addlegendentry{linear scaling};
      \addplot[dashed,black] coordinates {
        (16,8*5)
        (147456,8*5/9168)
      };
      \addplot[dashed,black] coordinates {
        (16,64*5)
        (147456,64*5/9168)
      };
      \addplot[dashed,black] coordinates {
        (16,5*512)
        (147456,5*512/9168)
      };
      \addplot[dashed,black] coordinates {
        (16,5*4096)
        (147456,5*4096/9168)
      };
    \end{loglogaxis}
  \end{tikzpicture}
  \begin{tikzpicture}
    \begin{loglogaxis}[
      title style={at={(1,0.965)},anchor=north east,draw=black,fill=white,font=\scriptsize\bf},
      title={$256^3$ mesh, discontinuous $\mathcal Q_3$ elements},
      width=0.48\textwidth,
      height=0.5\textwidth,
      xlabel={Number of cores},
      x label style={at={(0.5,0.02)}},
      xtick={56,224,896,3584,14336},
      xticklabels={56,224,896,3584,14336},
      tick label style={font=\scriptsize},
      label style={font=\scriptsize},
      legend style={font=\scriptsize},
      legend pos=south west,
      xmin=28, xmax=14336,
      ymin=5e-3, ymax=15,
      grid
      ]
      \addplot[blue,mark=*,densely dashed] table[x={nprocs}, y={olddg16m}] {\scalingHSW};
      \addlegendentry{old, 16M cells};
      \addplot[blue,mark=o] table[x={nprocs}, y={newdg16m}] {\scalingHSW};
      \addlegendentry{new, 16M cells};
      \addplot[red,mark=square*,densely dashed] table[x={nprocs}, y={olddg2m}] {\scalingHSW};
      \addlegendentry{old, 2M cells};
      \addplot[red,mark=square] table[x={nprocs}, y={newdg2m}] {\scalingHSW};
      \addlegendentry{new, 2M cells};
      \addplot[gnuplot@green,mark=diamond*,densely dashed] table[x={nprocs}, y={olddg256k}] {\scalingHSW};
      \addlegendentry{old, 256k cells};
      \addplot[gnuplot@green,mark=diamond] table[x={nprocs}, y={newdg256k}] {\scalingHSW};
      \addlegendentry{new, 256k cells};
    \end{loglogaxis}
  \end{tikzpicture}
  \caption{Scaling of multigrid algorithms on SuperMUC.}
\label{fig:scaling_mg}
\end{figure}

The scalability of the improved geometric multigrid framework is shown in
Fig.~\ref{fig:scaling_mg}, including a combined strong and weak scaling plot
in the left panel using continuous $\mathcal Q_3$ elements with 57~million to
232~billion degrees of freedom for discretizing the Laplacian. Along each
line, the same problem size is solved with an increasing number of cores,
whereas different lines are a factor of eight apart and always start at
3.5~million degrees of freedom per core. Almost ideal scalability down to
approximately 0.1~seconds can be observed also on 147k~cores. The right panel
of Fig.~\ref{fig:scaling_mg} shows the effect of the aforementioned
algorithmic improvements on a setup with discontinuous DG elements, clearly
improving the latency of the multigrid V-cycle. The improved algorithms are
shown in the updated step-37 tutorial program.



\subsection{The \texttt{FE\_Enriched} class}

\verb!FE_Enriched! finite element implements a partition of unity finite element method (PUM) by Babuska and Melenk which enriches a standard finite element with an enrichment function multiplied with another (usually linear) finite element. This allows to include in the finite element space a priori knowledge about the partial differential equation being solved which in turn improves the local approximation properties of the spaces. The user can also use enriched and non-enriched finite elements in different parts of the domain. 
\verb|!DoFTools::make_hanging_node_constraints()| function can automatically make the resulting space $C^0$ continuous.
Existing \verb|SolutionTransfer| class can be used to transfer the solution during $h$\,-adaptive refinement from a coarse to a fine mesh under the condition that all child elements are also enriched.

\subsection{The \texttt{FE\_Series} namespace}

TODO: Denis

\subsection{Matrix-free operators}

Facilitate the usage of matrix-free method by providing a \verb!MatrixFreeOperator::Base! class,
which implements various interface to matrix-vector products, including necessary operations when used in
the context of the geometric multigrids, methods needed for usage within the linear operator as well as with Jacobi preconditioner.
The derived classes only need to implement \verb!apply_add()! method that is
used in the \verb!vmult()! functions, and a method to compute the diagonal entries of the underlying matrix.
The \verb!MatrixFreeOperator! namespace contains implementations of \verb!MatrixFreeOperators::LaplaceOperator! and
\verb!MatrixFreeOperators::MassOperator!.

The framework was also included in the updated step-37 tutorial program.

\subsection{Incompatible changes}

\subsubsection{incompatible change 1}

Switch default of Lagrange elements to Gauss-Lobatto

\subsubsection{Other incompatible changes}

No \texttt{long double} instantiations any more

ParameterGUI moved to separate repo

The
\href{https://www.dealii.org/8.5.0/doxygen/deal.II/changes_between_8_4_and_8_5.html}{file
  that lists all changes for this release} (see \cite{changes84})
lists another around 20
incompatible changes, but none of these should in fact be visible in
typical user codes. Some remove previously deprecated classes and
functions, and the majority change internal interfaces that are not
typically used in user codes.



\section{How to cite \dealii{}}\label{sec:cite}

In order to justify the work the developers of \dealii{} put into this
software, we ask that papers using the library reference one of the
\dealii{} papers. This helps us justify the effort we put into it.

There are various ways to reference \dealii{}. To acknowledge the use of a
particular version of the library, reference the present document. For up
to date information and bibtex snippets for this document see:
\begin{center}
 \url{https://www.dealii.org/publications.html}
\end{center}

% \begin{minipage}{0.9\textwidth}%no page break in here please
% \begin{verbatim}
% @article{dealII82,
%   title = {The {\tt deal.{I}{I}} Library, Version 8.2},
%   author = {W. Bangerth and T. Heister and L. Heltai
%    and G. Kanschat and M. Kronbichler and M. Maier
%    and B. Turcksin and T. D. Young},
%   journal = {preprint},
%   year = {2015},
% }
% \end{verbatim} %  journal = {arXiv preprint \url{http://arxiv.org/abs/TODO}},
% \end{minipage}

The original \texttt{\dealii{}} paper containing an overview of its
architecture is \cite{BangerthHartmannKanschat2007}. If you rely on specific
features of the library, please consider citing any of the following:
\begin{itemize}
 \item For geometric multigrid: \cite{Kanschat2004,JanssenKanschat2011};
 \item For distributed parallel computing: \cite{BangerthBursteddeHeisterKronbichler11};
 \item For $hp$ adaptivity: \cite{BangerthKayserHerold2007};
  \item For $PUM$ and enrichment of the FE space: \cite{Davydov2016};
 \item For matrix-free and fast assembly techniques:
   \cite{KronbichlerKormann2012};
 \item For computations on lower-dimensional manifolds:
   \cite{DeSimoneHeltaiManigrasso2009};
 \item For integration with CAD files and tools:
   \cite{HeltaiMola2015};
 \item For \texttt{LinearOperator} and \texttt{PackagedOperation} facilities:
   \cite{MaierBardelloniHeltai-2016-a,MaierBardelloniHeltai-2016-b}.
 \item For uses of the \texttt{WorkStream} interface:
   \cite{TKB16}.
\end{itemize}

\dealii{} can interface with many other libraries:
\begin{multicols}{2}
\begin{itemize}
\item ARPACK \cite{arpack}
\item BLAS, LAPACK
\item HDF5 \cite{hdf5}
\item METIS \cite{karypis1998fast}
\item MUMPS \cite{ADE00,MUMPS:1,MUMPS:2,mumps-web-page}
\item muparser \cite{muparser-web-page}
\item NetCDF \cite{rew1990netcdf}
\item OpenCASCADE \cite{opencascade-web-page}
\item p4est \cite{p4est}
\item PETSc \cite{petsc-user-ref,petsc-web-page}
\item SLEPc \cite{Hernandez:2005:SSF}
\item Threading Building Blocks \cite{Rei07}
\item Trilinos \cite{trilinos,trilinos-web-page}
\item UMFPACK \cite{umfpack}
\end{itemize}
\end{multicols}
Please consider citing the appropriate references if you use interfaces to these
libraries.

Older releases of \dealii{} can be cited as \cite{dealII80,dealII81,dealII82,dealII83,dealII84}.

\nocite{BangerthKanschat1999}

\section{Acknowledgments}

\dealii{} is a world-wide project with dozens of contributors around the
globe. Other than the authors of this paper, the following people contributed code to
this release:
%
% get this from the changes/*/* files using the command listed in the
% release-tasks paper and remove the authors of this paper
%
Rajat  Arora,
Mauro  Bardelloni,
Conrad  Clevenger,
Sam  Cox,
Juliane  Dannberg,
Ren{\'e}  Gassm{\"o}ller,
Joscha  Gedicke,
Sebastian  Gonzalez-Pintor,
Ryan  Grove,
Michael  Harmon,
Daniel  Jodlbauer,
Guido  Kanschat,
Justin  Kauffman,
Paul  Kuberry,
Dustin  Kumor,
Konstantin  Ladutenko,
Andrew  McBride,
Mathias  Mentler,
Andrea  Mola,
Dragan  Nikolic,
Vaibhav  Palkar,
Spencer  Patty,
Jonathan  Perry-Houts,
Giuseppe  Pitton,
Ce  Qin,
Jonathan  Robey,
Mayank  Sabharwal,
Ali  Samii,
Alberto  Sartori,
Daniel  Shapero,
Martin  Steigemann,
Jihuan  Tian,
Jaeryun  Yim,
Toby  Young,
Zhao, Liang.

Their contributions are much appreciated!


\bigskip

\dealii{} and its developers are financially supported through a
variety of funding sources:

\marginpar{Add your funding here as appropriate}
W.~Bangerth was partially
supported by the National Science Foundation under award OCI-1148116
as part of the Software Infrastructure for Sustained Innovation (SI2)
program; and by the Computational Infrastructure in Geodynamics initiative
(CIG), through the National Science Foundation under Awards
No.~EAR-0949446 and EAR-1550901 and The University of California -- Davis.

T.~Heister was partially supported by the Computational Infrastructure in
Geodynamics initiative (CIG), through the National Science Foundation
under Award No. EAR-0949446 and The University of California -- Davis, and National Science Foundation grant DMS1522191.

M.~Kronbichler was partially supported by the German Research Foundation (DFG)
under the project ``High-order discontinuous Galerkin for the exa-scale''
(ExaDG) within the priority program ``Software for Exascale Computing''
(SPPEXA), grant agreement no.~KR4661/2-1, the Bayerisches Kompetenznetzwerk
f\"ur Technisch-Wissenschaftliches Hoch- und H\"ochstleistungsrechnen
(KONWIHR), and the Gauss Centre for Supercomputing e.V.~by providing computing
time on the GCS Supercomputer SuperMUC at Leibniz Supercomputing Centre (LRZ)
through project id pr83te.

J-P.~Pelteret was supported by the European Research Council (ERC) through the Advanced Grant 289049 MOCOPOLY.

The Interdisciplinary Center for Scientific Computing (IWR) at Heidelberg University has provided
hosting services for the \dealii{} web page and the SVN archive.


\bibliography{paper}{}
\bibliographystyle{abbrv}

\end{document}
